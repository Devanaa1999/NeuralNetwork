# -*- coding: utf-8 -*-
"""Neural Network (Lanjutan Feature Extraction).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kPKo6udElj4802c7pJfm9baVg8zXemLt
"""

!git clone https://github.com/nuradilahf/feature-extraction-revision

import pandas as pd

df_clean = pd.read_csv('/content/feature-extraction-revision/clean.csv')
df = pd.concat([df_clean, df_clean], ignore_index=True)

df.head()

df.shape

df.Sentiment.value_counts()

import nltk

nltk.download('punkt_tab')

from nltk.tokenize import sent_tokenize

text = "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung tahu berkualitas dipadu keahlian memasak dipadu kretivitas jadilah warung yang menyajikan menu utama berbahan tahu ditambah menu umum lain sepe i ayam semuanya selera indonesia harga cukup terjangkau jangan melewati tahu bletoka nya tidak kalah dengan yang asli dari tegal"
text = sent_tokenize(text)
text

from nltk.tokenize import word_tokenize

text = "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung tahu berkualitas dipadu keahlian memasak dipadu kretivitas jadilah warung yang menyajikan menu utama berbahan tahu ditambah menu umum lain sepe i ayam semuanya selera indonesia harga cukup terjangkau jangan melewati tahu bletoka nya tidak kalah dengan yang asli dari tegal"
text = word_tokenize(text)
text

import re

text = "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung tahu berkualitas dipadu keahlian memasak dipadu kretivitas jadilah warung yang menyajikan menu utama berbahan tahu ditambah menu umum lain sepe i ayam semuanya selera indonesia harga cukup terjangkau jangan melewati tahu bletoka nya tidak kalah dengan yang asli dari tegal"
text = re.sub(r'[^\w\s]', '', text)
text

text = "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung tahu berkualitas dipadu keahlian memasak dipadu kretivitas jadilah warung yang menyajikan menu utama berbahan tahu ditambah menu umum lain sepe i ayam semuanya selera indonesia harga cukup terjangkau jangan melewati tahu bletoka nya tidak kalah dengan yang asli dari tegal"
text = re.sub(r'\d+', '', text)
text

stopwords = ["ini", "oleh", "yang", "sudah", "di", "cukup", "jadilah", "dari", "nya", "i"]
text = "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung tahu berkualitas dipadu keahlian memasak dipadu kretivitas jadilah warung yang menyajikan menu utama berbahan tahu ditambah menu umum lain sepe i ayam semuanya selera indonesia harga cukup terjangkau jangan melewati tahu bletoka nya tidak kalah dengan yang asli dari tegal"

words = text.split(' ')
for word in words:
  if word not in stopwords:
    print(word)

data_preprocessed = df.Text_Bersih.tolist()

data_preprocessed

import numpy as np

data_preprocessed = [x for x in data_preprocessed if x is not np.nan and x is not None and (isinstance(x, str) or isinstance(x, bytes) or (isinstance(x, float) and not pd.isna(x)))]

from sklearn.feature_extraction.text import CountVectorizer

count_vect = CountVectorizer()
count_vect.fit(data_preprocessed)

count_vect.vocabulary_

X = count_vect.transform(data_preprocessed)

X.shape

print (X)

print ("Feature Extraction selesai")

import pickle
with open("feature.p", "wb") as file:
  pickle.dump(count_vect, file)

from sklearn.model_selection import train_test_split
classes = df.Sentiment

classes

classes = df.loc[df['Text_Bersih'].notna(), 'Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, classes, test_size=0.2)

# Create DataFrames for train and test data
train_data = pd.DataFrame(X_train.toarray())  # Convert X_train to a DataFrame
train_data['Sentiment'] = y_train  # Add the target variable

test_data = pd.DataFrame(X_test.toarray())  # Convert X_test to a DataFrame
test_data['Sentiment'] = y_test   # Add the target variable

train_data.to_csv('train_data.csv', index=False)
test_data.to_csv('test_data.csv', index=False)

print("Train and Test data have been exported successfully!")

"""Proses Neural Network"""

from sklearn.neural_network import MLPClassifier
model = MLPClassifier()
model. fit(X_train, y_train)
print ("Training selesai")

pickle. dump(model, open ("model-p", "wb"))

from sklearn.metrics import classification_report
test = model.predict(X_test)
print ("Testing selesai")
print(classification_report(y_test, test))
print(classification_report(y_test, test))

import numpy as np
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, random_state=42, shuffle=True)

accuracies = []

y = classes  # Pastikan 'classes' sudah didefinisikan sebelumnya

for iteration, data in enumerate(kf.split(X), start=1):
    # Use .iloc to access data by position
    data_train = X[data[0]]
    target_train = y.iloc[data[0]]  # Use .iloc for positional indexing

    data_test = X[data[1]]
    target_test = y.iloc[data[1]]  # Use .iloc for positional indexing

    clf = MLPClassifier()
    clf.fit(data_train, target_train)

    preds = clf.predict(data_test)

    accuracy = accuracy_score(target_test, preds)

    print("Training ke-", iteration)
    print(classification_report(target_test, preds))
    print("======================================")

    accuracies.append(accuracy)

average_accuracy = np.mean(accuracies)

print()
print()
print("Rata-rata Accuracy:", average_accuracy)

import re

def cleansing(text):
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Lowercase the text
    text = text.lower()
    return text

original_text = '''
Rasa syukur, cukup.
'''

text = count_vect.transform([cleansing(original_text)])

result = model.predict(text)[0]
print("Sentiment:")
print()
print(result)

filename_model = 'trained_model.pkl'
filename_vectorizer = 'trained_vectorizer.pkl'

pickle.dump(model, open(filename_model, 'wb'))
pickle.dump(count_vect, open(filename_vectorizer, 'wb'))

print(f"Model saved to: {filename_model}")
print(f"Vectorizer saved to: {filename_vectorizer}")

print("Neural Network Selesai")

